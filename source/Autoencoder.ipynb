{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Imports and Determine Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Define Imports and Determine Device\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "os.sys.path.insert(0, \".\")\n",
    "from DiabetesData import DiabeticData\n",
    "from Autoencoder import ConvAutoencoder\n",
    "from ConvVarAutoencoder import ConvVAE\n",
    "from DogCatData import DogCatData\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "datatype=\"dogcat\"\n",
    "batch_size=16\n",
    "epochs = 5\n",
    "\n",
    "normalize=False\n",
    "size=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'dog': 12500, 'cat': 3030})\n",
      "Counter({0: 12500, 1: 3030})\n",
      "           filename  label text_label\n",
      "10403  dog.8110.jpg      0        dog\n",
      "3714   dog.2090.jpg      0        dog\n",
      "3427   dog.1832.jpg      0        dog\n",
      "3693   dog.2071.jpg      0        dog\n",
      "3899   dog.2257.jpg      0        dog\n",
      "...             ...    ...        ...\n",
      "9624    dog.741.jpg      0        dog\n",
      "12078  dog.9619.jpg      0        dog\n",
      "10735   dog.841.jpg      0        dog\n",
      "5552   dog.3745.jpg      0        dog\n",
      "3811   dog.2178.jpg      0        dog\n",
      "\n",
      "[10126 rows x 3 columns]\n",
      "       index      filename  label text_label\n",
      "0      10403  dog.8110.jpg      0        dog\n",
      "1       3714  dog.2090.jpg      0        dog\n",
      "2       3427  dog.1832.jpg      0        dog\n",
      "3       3693  dog.2071.jpg      0        dog\n",
      "4       3899  dog.2257.jpg      0        dog\n",
      "...      ...           ...    ...        ...\n",
      "10121   9624   dog.741.jpg      0        dog\n",
      "10122  12078  dog.9619.jpg      0        dog\n",
      "10123  10735   dog.841.jpg      0        dog\n",
      "10124   5552  dog.3745.jpg      0        dog\n",
      "10125   3811  dog.2178.jpg      0        dog\n",
      "\n",
      "[10126 rows x 4 columns]\n",
      "      index       filename  label text_label\n",
      "0      4561   dog.2853.jpg      0        dog\n",
      "1      6199   dog.4327.jpg      0        dog\n",
      "2      4843   dog.3106.jpg      0        dog\n",
      "3      8181   dog.6110.jpg      0        dog\n",
      "4      9795   dog.7564.jpg      0        dog\n",
      "...     ...            ...    ...        ...\n",
      "1393  11165   dog.8798.jpg      0        dog\n",
      "1394   4726   dog.3000.jpg      0        dog\n",
      "1395    336    dog.103.jpg      0        dog\n",
      "1396  15154  cat.12386.jpg      1        cat\n",
      "1397   1583  dog.11421.jpg      0        dog\n",
      "\n",
      "[1398 rows x 4 columns]\n",
      "      index       filename  label text_label\n",
      "0      5909   dog.4066.jpg      0        dog\n",
      "1       235  dog.10208.jpg      0        dog\n",
      "2      3934   dog.2289.jpg      0        dog\n",
      "3      8118   dog.6054.jpg      0        dog\n",
      "4      4366   dog.2678.jpg      0        dog\n",
      "...     ...            ...    ...        ...\n",
      "1548   7073   dog.5113.jpg      0        dog\n",
      "1549   6210   dog.4337.jpg      0        dog\n",
      "1550   1087  dog.10976.jpg      0        dog\n",
      "1551    296  dog.10263.jpg      0        dog\n",
      "1552  12716  cat.10191.jpg      1        cat\n",
      "\n",
      "[1553 rows x 4 columns]\n",
      "(10126, 4)\n",
      "(1398, 4)\n",
      "(1553, 4)\n"
     ]
    }
   ],
   "source": [
    "if datatype == \"diabetes\":\n",
    "    filename = \"data/trainLabels_ad.csv\"\n",
    "    root_dir = \"data/diabetes_resized\"\n",
    "    task = ([0],[4])\n",
    "    # task = ([0,1,2], [3,4])\n",
    "    # task = ([0,1,2], (3,4))\n",
    "\n",
    "    train = pd.read_csv(\"data/diabetes_ad_train.csv\")\n",
    "    valid = pd.read_csv(\"data/diabetes_ad_valid.csv\")\n",
    "    test = pd.read_csv(\"data/diabetes_ad_test.csv\")\n",
    "\n",
    "    data = {'train': DiabeticData(df = train, transform_key=\"train\", root_dir=root_dir, task = task, normalize = normalize),\n",
    "            'valid': DiabeticData(df = val, transform_key=\"valid\", root_dir=root_dir, task = task, normalize = normalize),\n",
    "            'test': DiabeticData(df = test, transform_key=\"test\", root_dir=root_dir, task = task, normalize = normalize)\n",
    "            }\n",
    "\n",
    "\n",
    "elif datatype == \"dogcat\":\n",
    "    filename = \"data/dogcat_ad.csv\"\n",
    "    root_dir = \"data/dogcat/train\"\n",
    "    classes = ['dog', 'cat']\n",
    "    # task = ([0,1,2], (3,4))\n",
    "\n",
    "    train = pd.read_csv(\"data/dogcat_ad_train.csv\")\n",
    "    valid = pd.read_csv(\"data/dogcat_ad_valid.csv\")\n",
    "    test = pd.read_csv(\"data/dogcat_ad_test.csv\")\n",
    "\n",
    "    data = {'train': DogCatData(df = train, transform_key=\"train\", root_dir=root_dir, normalize = normalize),\n",
    "            'valid': DogCatData(df = val, transform_key=\"valid\", root_dir=root_dir, normalize = normalize),\n",
    "            'test': DogCatData(df = test, transform_key=\"test\", root_dir=root_dir, normalize = normalize)\n",
    "            }\n",
    "\n",
    "dataloaders = {\n",
    "        'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
    "        'valid': DataLoader(data['valid'], batch_size=batch_size, shuffle=True),\n",
    "        'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0): Conv2d(3, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0): ConvTranspose2d(4, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(1024, 3, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (criterion): BCELoss()\n",
      ")\n",
      "in fit function\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [03:08<00:00,  3.37it/s]\n",
      "  0%|          | 0/633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 9.079327\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [03:06<00:00,  3.40it/s]\n",
      "  0%|          | 0/633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 9.005630\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [03:06<00:00,  3.39it/s]\n",
      "  0%|          | 0/633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 8.996671\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 578/633 [02:50<00:16,  3.24it/s]"
     ]
    }
   ],
   "source": [
    "model = ConvAutoencoder(device, datatype)\n",
    "print(model)\n",
    "\n",
    "model.fit(epochs, dataloaders[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# obtain one batch of test images\n",
    "dataiter = iter(dataloaders[\"test\"])\n",
    "images, labels = dataiter.next()\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images.to(device))\n",
    "# output = F.softmax(output)\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "\n",
    "# output is resized into a batch of iages\n",
    "output = output.view(batch_size, 3, size, size)\n",
    "# use detach when it's an output that requires_grad\n",
    "output = output.cpu().detach().numpy()\n",
    "\n",
    "# # plot the first ten input images and then reconstructed images\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "\n",
    "# # input images on top row, reconstructions on bottom\n",
    "# for images, row in zip([images, output], axes):\n",
    "#     for img, ax in zip(images, row):\n",
    "#         ax.imshow(np.squeeze(img))\n",
    "#         ax.get_xaxis().set_visible(False)\n",
    "#         ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(output[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "\n",
    "\n",
    "# plot the first ten input images and then reconstructed images\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we loop through the training set and calculate reconstruction loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders['valid'] = DataLoader(data['valid'], batch_size=1, shuffle=True)\n",
    "dataloaders['test'] = DataLoader(data['test'], batch_size=1, shuffle=True)\n",
    "\n",
    "results = []\n",
    "results_cols = [\"Image Label\", \"Reconstruction Loss\"]\n",
    "for x, y in dataloaders['valid']:\n",
    "    X = x.to(device)\n",
    "    output = model(X)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for i in range(y.shape[0]):\n",
    "        ls = 0\n",
    "        image = x[i].numpy()\n",
    "        ouptut = output[i]\n",
    "        label = y[i].numpy()\n",
    "        ls = np.sum(np.square(image.ravel() - output.ravel()))\n",
    "        # ls = model.criterion(output, image)\n",
    "        results.append([label, ls])\n",
    "\n",
    "results = pd.DataFrame(results, columns=results_cols)\n",
    "results.to_csv(\"reconstruction_error.csv\")\n",
    "\n",
    "\n",
    "#find error threshold on validation set\n",
    "\n",
    "\n",
    "\n",
    "#evaluate on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "label_1 = results[results[\"Image Label\"] == 1]\n",
    "label_0 = results[results[\"Image Label\"] == 0]\n",
    "print(label_0)\n",
    "print(label_1)\n",
    "avg_1 = np.mean(label_1['Reconstruction Loss'].values)\n",
    "avg_0 = np.mean(label_0['Reconstruction Loss'].values)\n",
    "\n",
    "print(\"Average Reconstruction Error (Prediction = 0)\", avg_0)\n",
    "print(\"Average Reconstruction Error (Prediction = 1)\", avg_1)\n",
    "\n",
    "plt.hist(label_1['Reconstruction Loss'].values, density=False, bins=30, color='blue')\n",
    "plt.hist(label_0['Reconstruction Loss'].values, density=False, bins=30, alpha = 0.5,color='yellow')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 750\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for x, y in dataloaders['test']:\n",
    "    X = x.to(device)\n",
    "    output = model(X)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    for i in range(y.shape[0]):\n",
    "        ls = 0\n",
    "        image = x[i].numpy()\n",
    "        ouptut = output[i]\n",
    "        label = y[i].numpy()\n",
    "        ls = np.sum(np.square(image.ravel() - output.ravel()))\n",
    "        y_true.append(label)\n",
    "        if ls >= threshold:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", rec)\n",
    "print(\"F1-Score: \", f1)\n",
    "print(\"Confusion Matrix: \", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}